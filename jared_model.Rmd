---
title: "jared_model"
author: "Jared Petry"
date: "2023-03-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For this quarter's final lab, you will be using the knowledge of machine learning that you have gained this quarter to training models to predict dissolved inorganic carbon in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI).

## Your Task:
- Acquire domain knowledge. (Dr. Satterthwaite's presentation)
- Explore the data.
- Preprocessing.
- Choose a model algorithm.
- Tune relevant parameters (Cross validation).
- Submit your prediction.

```{r include=FALSE}
library(caret) 
library(tidymodels) 
library(tidyverse)
library(dplyr) 
library(gbm)
library(xgboost)
library(ggpubr)
library(tictoc)
library(vip)
library(Hmisc)
```


## Read in the data: 

This dataset was downloaded from the CalCOFI data portal. Bottle and cast data was downloaded and merged, then relevant variables were selected.

You will use the data contained in the train.csv file to train a model that will predict dissolved inorganic carbon (DIC) content in the water samples.

```{r}
dic_train <- read_csv("train.csv") |> select(-...13) # take out blank column
dic_test <- read_csv("test.csv")
```

Here's a short description of each variables: 

NO2uM - Micromoles Nitrite per liter of seawater
NO3uM - Micromoles Nitrate per liter of seawater
NH3uM - Micromoles Ammonia per liter of seawater
R_TEMP - Reported (Potential) Temperature in degrees Celsius
R_Depth - Reported Depth (from pressure) in meters
R_Sal - Reported Salinity (from Specific Volume Anomoly, MÂ³/Kg)
R_DYNHT - Reported Dynamic Height in units of dynamic meters (work per unit mass)
R_Nuts - Reported Ammonium concentration
R_Oxy_micromol.Kg - Reported Oxygen micromoles/kilogram
PO4uM - Micromoles Phosphate per liter of seawater
SiO3uM - Micromoles Silicate per liter of seawater
TA1.x - Total Alkalinity micromoles per kilogram solution
Salinity1 - Salinity
Temperature_degC - Temp
DIC - Outcome

**Heads up: one variable is differing in the train set: TA1.x in train set and TA1 in test set... Also column 13 is blank so I am removing it from the start

### Data exploration: 

```{r}
# let's see which variables are most correlated with variable DIC 
corr <- rcorr(as.matrix(dic_train))
corr$r

```
As we can see from this correlation matrix, the variables that are highly correlated with DIC are: 
- NO3uM
- R_TEMP
- R_Depth
- R_Sal
- R_DYNHT
- R_Oxy_micromol.Kg 
- PO4uM 
- SiO3uM 
- TA1.x 
- Salinity1

Variables that are NOT highly correlated with DIC: 
- id 
- Lat_Dec 
- Lon_Dec 
- NO2uM
- NH3uM
- R_Nuts



## Pick a model: 

Here are the types of models that we have worked with in this class: 
linear regression 
- lab 1: used to predict the price of a pumpkin (linear_reg() function)
regularized multivariate polynomial regression 
- used the same as above with step_poly and poly_spec?? I am confused how different this would be or if this would be possible with many variables
classification and logistic regression 
- not applicable here because we are predicting a continuous variable
k nearest neighbors 
- could be a good option?

decision tree 
bagged forest
random forest 
boosted trees

We are not doing classification, we are trying to predict an actual number for DIC, so 

### Let's try a K-nearest neighbor model 

```{r}
# split the training data 

# create a recipe
dic_recipe <- recipe(DIC ~ ., data = dic_train) |> 
  step_normalize(all_numeric(), -all_outcomes()) |> 
  prep() 
# potentially add some pre-processing steps?? all the data are already numeric
# I think the knn model like normalized data

# bake the recipe
baked_dic <- bake(dic_recipe, dic_train)

knn_spec_tune <- nearest_neighbor(neighbors = tune()) |> 
  set_engine("kknn") |> 
  set_mode("regression")
knn_spec_tune

# create cross validation folds 
set.seed(1738)
folds <- vfold_cv(data = dic_train, 
                  v = 10,
                  strata = DIC)

# build a workflow
knn_workflow <- workflow() |> 
  add_model(knn_spec_tune) |> 
  add_recipe(dic_recipe) 

# now fit the resamples
set.seed(345)
fit_knn_cv <- knn_workflow |> 
  tune_grid(
    folds,
    grid = data.frame(neighbors = c(1, 5, 10, seq(20, 100, 10))) # this will make it try running on all different folds, for example we are keeping it                                             simple for run time. sample different from 1-100
  )

# Check the performance with collect_metrics()
fit_knn_cv %>% collect_metrics()

# show the best number of neighborsfor fitting to our cv folds
show_best(fit_knn_cv)
# ---- looks like 10 neighbors is best!

# now we can finalize our workflow
final_wf <- 
  knn_workflow |> 
  finalize_workflow(select_best(fit_knn_cv))
final_wf

# make a final fit 
final_fit <- final_wf |> last_fit(dic_test)
# finally, make predictions on the test data  
dic_pred_on_test <- predict(k_10_mod, dic_test)

# welp, this code doesn't run but I'm getting like over 10 RMSE and that's not very good so I don't think KNN is the way to go since there's not anything left for me to tune other than K
```





