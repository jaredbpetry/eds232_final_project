---
title: "Boosted Trees Model"
author: "Hailey Veirs"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rsample)
library(caret)
library(recipes)
library(tidymodels)
library(xgboost)
library(tictoc)
library(vip)
#library(Ckmeans.1d.dp)
```

```{r}
# Load in the training data
train_data <- read_csv(file = "train.csv") %>% select(-`...13`)
```

## Objective:
Predict Dissolved Inorganic Carbon (DIC) in water samples.

### Preprocessing
Divide the training data (train.csv), apply a preprocessing recipe, and then take resamples.

```{r}
set.seed(31923)
# Make and apply a recipe
data <- recipe(DIC ~ ., data = train_data) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  prep() %>% 
  bake(., train_data)
```

```{r}
set.seed(31923)
# Now split the data further
split <- initial_split(data)
train_extra <- training(split)
test_extra <- testing(split)
```

```{r}
set.seed(31923)
# And take CV resamples
train_cv <- data %>% vfold_cv(v = 10)
train_cv_extra <- train_extra %>% vfold_cv(v = 10)
```

## Model and Tuning Parameters
### First model, Tune the learning rate

```{r}
set.seed(31923)
xgboost <- boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = 6,
    tree_depth = 3,
    learn_rate = tune(),
    loss_reduction = 0.1, 
    mtry = 3,
    sample_size = 0.40,
    stop_iter = 5
    ) %>%
    set_engine("xgboost")
```

```{r}
set.seed(31923)
doParallel::registerDoParallel()
tic()
# Create a grid to tune the learn_rate first:
learn_rt_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

learn_rt_tune1 <- tune_grid(xgboost, 
                     DIC ~ ., 
                     resamples = train_cv, 
                     grid = learn_rt_grid,
                     metrics = metric_set(rmse))
toc()
```

```{r}
# I can use autoplot to plot the tuning grid
autoplot(learn_rt_tune1) + theme_light()
show_best(learn_rt_tune1)
select_best(learn_rt_tune1)
```

### Tune the tree parameters

```{r}
set.seed(31923)
# Next tune the tree parameters:
xgboost2 <- boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = select_best(learn_rt_tune1),
    loss_reduction = tune(), 
    mtry = 3,
    sample_size = 0.40,
    stop_iter = 5
    ) %>%
    set_engine("xgboost")
```

```{r}
# Set the parameters:
tree_params <- parameters(min_n(), tree_depth(), loss_reduction())

# Set the workflow
tree_wf <- workflow() %>%
  add_model(xgboost2) %>% 
  add_formula(DIC ~ .)
```


```{r}
set.seed(31923)
doParallel::registerDoParallel()
tic()
# Create a grid to tune the tree parameters too:
tree_param_grid <- grid_max_entropy(tree_params, iter = 500, size = 60)

tree_param_tune <- tune_grid(object = tree_wf, 
                     resamples = train_cv, 
                     grid = tree_param_grid,
                     metrics = metric_set(rmse),
                     control = control_grid(verbose = TRUE))
toc()
```

```{r}
# Use autoplot
autoplot(tree_param_tune) + theme_light()
show_best(tree_param_tune)
best_tree <- select_best(tree_param_tune)
```

### Stocastic Parameter tuning

```{r}
set.seed(31923)

xgboost3 <- boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = best_tree$min_n,
    tree_depth = best_tree$tree_depth,
    learn_rate = select_best(learn_rt_tune1),
    loss_reduction = best_tree$loss_reduction, 
    mtry = tune(),
    sample_size = tune(),
    stop_iter = 5
    ) %>%
    set_engine("xgboost")

# To use the grid_max_entropy, need to use the parameters on the specification:
stochastic_params <- parameters(finalize(mtry(), select(data, -DIC)),
                                sample_size = sample_prop(range = c(4/10, 9/10)))
```

```{r}
stochastic_wf <- workflow() %>%
  add_model(xgboost3) %>% 
  add_formula(DIC ~ .)
```

```{r}
set.seed(31923)
doParallel::registerDoParallel()
tic()
# Create a grid to tune the tree parameters too:
stochastic_grid <- grid_max_entropy(stochastic_params, iter = 500, size = 60)

stochastic_tune <- tune_grid(object = stochastic_wf, 
                             resamples = train_cv, 
                             grid = stochastic_grid, 
                             metrics = metric_set(rmse), 
                             control = control_grid(verbose = TRUE))
toc()
```

```{r}
# Use autoplot
autoplot(stochastic_tune) + theme_light()
show_best(stochastic_tune)
best_sto <- select_best(stochastic_tune)
```

### Final model

```{r}
set.seed(31923)
# Retune the learning rate
xgboost4 <- boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = best_tree$min_n,
    tree_depth = best_tree$tree_depth,
    learn_rate = tune(),
    loss_reduction = best_tree$loss_reduction,
    mtry = best_sto$mtry, 
    sample_size = best_sto$sample_size,
    stop_iter = 5
    ) %>%
    set_engine("xgboost")
```

```{r}
set.seed(31923)
# Retune the learn_rate for the final model
doParallel::registerDoParallel()
tic()
# Create a grid to tune the learn_rate first:
learn_rt_grid2 <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

learn_rt_tune2 <- tune_grid(xgboost4, 
                     DIC ~ ., 
                     resamples = train_cv, 
                     grid = learn_rt_grid2,
                     metrics = metric_set(rmse))
toc()
```

```{r}
# Then find the best learn_rate for the final workflow
autoplot(learn_rt_tune2) + theme_light()
show_best(learn_rt_tune2)
select_best(learn_rt_tune2)
```

```{r}
set.seed(31923)
# Finalize the model
xgb_final <- finalize_model(xgboost4, select_best(learn_rt_tune2))

recipe <- recipe(DIC ~ ., data = data) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
  step_normalize(all_numeric(), -all_outcomes())

# Set the final workflow
final_wf <- workflow() %>%
  add_model(xgb_final) %>%
  add_recipe(recipe = recipe)
```

```{r}
set.seed(31923)
# Make a final fit of the model with the data
train_fit <- xgb_final %>% fit(DIC ~ ., data = data)
train_fit
```

```{r}
#train_test_fit <- xgb_final %>% fit(DIC ~ ., data = test_extra)
#train_test_fit
```


```{r}
#set.seed(31923)
# I can also get predictions for the testing data to better see how the model evaluates.
#test_pred <- train_fit %>% predict(new_data = test_extra)
#test_prob <- train_fit %>% 
#  predict_numeric.model_fit(new_data = test_extra)
#proc_mw <- bind_cols(test_extra, test_pred, test_prob)

# check the best iteration
#proc_mw %>% metrics(`...20`, truth = DIC, estimate = .pred)
```

## Predict on New Data (test.csv)

```{r}
set.seed(31923)
# First, load and prep the new data
new_data <- read_csv("test.csv") %>% rename(TA1.x = TA1) #%>% mutate(DIC = as.numeric(NA))

#rec_new <- recipe(DIC ~ ., data = new_data) %>% 
#  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% 
#  step_normalize(all_numeric(), -all_outcomes())

#bake_new <- rec_new %>% prep() %>% bake(., new_data)
```

```{r}
set.seed(31923)
# Now I can fit my model to the new data:
test_eval_fit <- predict(train_fit, new_data = new_data)
test_eval_prob <- train_fit %>% 
  predict_numeric.model_fit(new_data = new_data)

proc_mw2 <- bind_cols(new_data, test_eval_fit, test_eval_prob) %>% 
  rename(test.id = id) %>% rename(DIC = .pred) %>%
  select(test.id, DIC)

# check the best iteration
#proc_mw2 %>% metrics(`...19`, estimate = .pred)
```

```{r}
# Now save the resulting predictions to a new csv file (un-comment the line below if you want to create the csv file)
#write.csv(proc_mw2, file = "submission.csv", row.names = FALSE)
```


